"""
LLM å¤„ç†æ¨¡å—ï¼šè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ API
"""
import requests
from typing import Optional, Tuple, List, Dict

from .config import PodcastConfig
from .parser import smart_parse_script


def call_llm_step(
    config: PodcastConfig,
    prompt: str,
    content: str,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> Optional[str]:
    """
    è°ƒç”¨ LLM API
    
    Args:
        config: é…ç½®å¯¹è±¡
        prompt: ç³»ç»Ÿæç¤ºè¯
        content: ç”¨æˆ·è¾“å…¥å†…å®¹
        temperature: ç”Ÿæˆæ¸©åº¦
        max_tokens: æœ€å¤§ token æ•°
        
    Returns:
        LLM è¿”å›çš„å†…å®¹ï¼Œå¤±è´¥è¿”å› None
    """
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {config.api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": config.llm_model_name,
        "messages": [
            {"role": "system", "content": prompt},
            {"role": "user", "content": content}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(url, json=payload, headers=headers, timeout=60)
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result:
                return result['choices'][0]['message']['content']
        print(f"âŒ LLM API Error: {response.text}")
        return None
    except Exception as e:
        print(f"âŒ LLM Exception: {e}")
        return None


def process_article(
    config: PodcastConfig,
    index: int,
    url: str,
    raw_text: str
) -> Tuple[int, str, Optional[str], Optional[List[Dict[str, str]]]]:
    """
    å¤„ç†å•ç¯‡æ–‡ç« ï¼šåˆ†æ + ç”Ÿæˆè„šæœ¬
    
    Args:
        config: é…ç½®å¯¹è±¡
        index: æ–‡ç« ç´¢å¼•
        url: æ–‡ç«  URL
        raw_text: æ–‡ç« åŸå§‹å†…å®¹
        
    Returns:
        (index, url, readable_script, script_json) å…ƒç»„
    """
    if not raw_text:
        return index, url, None, None

    prompts = config.get_prompts()
    
    print(f"ğŸ§  [Task {index+1}] LLM Analyzing...")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†ææ–‡ç« 
    analysis = call_llm_step(
        config,
        prompts["analyst"],
        raw_text[:10000]
    )
    
    if not analysis:
        return index, url, None, None
    
    # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè„šæœ¬
    script_raw = call_llm_step(
        config,
        prompts["playwright"],
        f"ã€ç®€æŠ¥ã€‘ï¼š\n{analysis}"
    )
    
    # è§£æè„šæœ¬
    script_json = smart_parse_script(script_raw)
    
    if not script_json:
        print(f"\n{'!'*40}")
        print(f"ğŸ•µï¸â€â™‚ï¸ [DEBUG Task {index+1}] æ ¼å¼ä¾ç„¶é”™è¯¯ï¼Œè¯·æ£€æŸ¥ Prompt")
        print(f"ğŸ“œ åŸå§‹è¿”å›:\n{script_raw[:500] if script_raw else 'None'}")
        print(f"{'!'*40}\n")
        return index, url, None, None
    
    # ç”Ÿæˆå¯è¯»æ–‡æœ¬
    readable_script = f"Source: {url}\n\n"
    for line in script_json:
        spk = line['speaker']
        txt = line['text']
        readable_script += f"{spk}: {txt}\n"
    readable_script += "\n" + "="*20 + "\n\n"
    
    print(f"âœ… [Task {index+1}] Script Ready ({len(script_json)} lines).")
    return index, url, readable_script, script_json
